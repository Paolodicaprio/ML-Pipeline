"""
Application Streamlit pour visualiser les r√©sultats des mod√®les de machine learning.
"""
import os
import sys
import json
import base64
import datetime
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
from PIL import Image

# Configuration de la page
st.set_page_config(
    page_title="ML Pipeline - Tableau de bord",
    page_icon="üìä",
    layout="wide"
)

# Titre et description
st.title("üìä ML Pipeline - Tableau de bord")
st.write("""
Cette application pr√©sente les r√©sultats des mod√®les de machine learning ex√©cut√©s par le pipeline CI/CD.
Elle permet de visualiser les performances des mod√®les, de comparer les versions et de suivre l'√©volution des m√©triques.
""")

# Fonction pour charger les donn√©es
@st.cache_data
def load_data(path):
    """Charge les donn√©es JSON depuis un fichier."""
    try:
        with open(path, 'r') as f:
            data = json.load(f)
        return data
    except Exception as e:
        st.error(f"Erreur lors du chargement des donn√©es: {e}")
        return None

# Fonction pour convertir une image base64 en image PIL
def base64_to_image(base64_str):
    """Convertit une cha√Æne base64 en image PIL."""
    try:
        img_data = base64.b64decode(base64_str)
        return Image.open(BytesIO(img_data))
    except Exception as e:
        st.error(f"Erreur lors de la conversion de l'image: {e}")
        return None

# Barre lat√©rale pour la navigation
st.sidebar.title("Navigation")
page = st.sidebar.radio(
    "Choisir une page",
    ["üìà Tableau de bord", "üîç D√©tails du mod√®le", "üîÑ Comparaison", "‚ÑπÔ∏è √Ä propos"]
)

# Page: Tableau de bord
if page == "üìà Tableau de bord":
    st.header("üìà Tableau de bord")
    
    # Rechercher les rapports disponibles
    reports_path = "deploy"
    if os.path.exists(reports_path):
        st.success("Mod√®les d√©ploy√©s trouv√©s!")
        
        # Charger les m√©tadonn√©es du mod√®le
        metadata = load_data(os.path.join(reports_path, "model_metadata.json"))
        if metadata:
            # Afficher les informations du mod√®le
            st.subheader("üìã Informations sur le mod√®le")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Nom du mod√®le", metadata['model_name'])
            with col2:
                st.metric("Type de mod√®le", metadata['model_type'].capitalize())
            with col3:
                st.metric("Version", metadata['model_version'])
            
            # Charger le rapport d'√©valuation
            evaluation = load_data(os.path.join(reports_path, "evaluation_report.json"))
            if evaluation:
                # Afficher les m√©triques principales
                st.subheader("üìä M√©triques principales")
                metrics = evaluation['metrics']
                
                # Cr√©er une mise en page en colonnes pour les m√©triques
                cols = st.columns(len(metrics))
                for i, (name, value) in enumerate(metrics.items()):
                    with cols[i]:
                        if name.lower() in ['mse', 'mae', 'rmse']:  # M√©triques d'erreur (plus bas = mieux)
                            st.metric(name.upper(), f"{value:.4f}", "Erreur")
                        else:  # M√©triques de performance (plus haut = mieux)
                            st.metric(name.capitalize(), f"{value:.4f}", "")
                
                # Visualisations
                if 'visualizations' in evaluation:
                    st.subheader("üìâ Visualisations")
                    visualizations = evaluation['visualizations']
                    
                    # Afficher les visualisations selon le type de mod√®le
                    if metadata['model_type'] == 'classification':
                        if 'confusion_matrix' in visualizations:
                            st.image(base64_to_image(visualizations['confusion_matrix']), caption="Matrice de confusion")
                        
                        if 'roc_curve' in visualizations and 'pr_curve' in visualizations:
                            col1, col2 = st.columns(2)
                            with col1:
                                st.image(base64_to_image(visualizations['roc_curve']), caption="Courbe ROC")
                            with col2:
                                st.image(base64_to_image(visualizations['pr_curve']), caption="Courbe Pr√©cision-Rappel")
                                
                    elif metadata['model_type'] == 'regression':
                        if 'scatter_plot' in visualizations and 'error_histogram' in visualizations:
                            col1, col2 = st.columns(2)
                            with col1:
                                st.image(base64_to_image(visualizations['scatter_plot']), caption="Pr√©dictions vs Valeurs r√©elles")
                            with col2:
                                st.image(base64_to_image(visualizations['error_histogram']), caption="Distribution des erreurs")
        else:
            st.warning("Aucune m√©tadonn√©e de mod√®le trouv√©e.")
    else:
        st.warning("Aucun mod√®le d√©ploy√© trouv√©. Veuillez ex√©cuter le pipeline complet pour d√©ployer un mod√®le.")

# Page: D√©tails du mod√®le
elif page == "üîç D√©tails du mod√®le":
    st.header("üîç D√©tails du mod√®le")
    
    # V√©rifier si un mod√®le est d√©ploy√©
    deploy_path = "deploy"
    if os.path.exists(deploy_path):
        # Charger les m√©tadonn√©es du mod√®le
        metadata = load_data(os.path.join(deploy_path, "model_metadata.json"))
        if metadata:
            # Informations d√©taill√©es sur le mod√®le
            st.subheader("üîé Param√®tres du mod√®le")
            st.json(metadata)
            
            # Afficher les param√®tres d'entra√Ænement
            if 'training_params' in metadata:
                st.subheader("‚öôÔ∏è Param√®tres d'entra√Ænement")
                for param, value in metadata['training_params'].items():
                    st.text(f"{param}: {value}")
                    
            # Historique des m√©triques
            st.subheader("üìù Historique des m√©triques")
            if 'validation_metrics' in metadata:
                validation_metrics = metadata['validation_metrics']
                df_val = pd.DataFrame({
                    'M√©trique': list(validation_metrics.keys()),
                    'Validation': list(validation_metrics.values())
                })
                
                # Charger le rapport d'√©valuation pour les m√©triques de test
                evaluation = load_data(os.path.join(deploy_path, "evaluation_report.json"))
                if evaluation and 'metrics' in evaluation:
                    test_metrics = evaluation['metrics']
                    df_val['Test'] = [test_metrics.get(metric, None) for metric in validation_metrics.keys()]
                
                # Afficher le tableau des m√©triques
                st.dataframe(df_val)
                
                # Graphique des m√©triques
                fig, ax = plt.subplots(figsize=(10, 6))
                df_melt = pd.melt(df_val, id_vars=['M√©trique'], value_vars=['Validation', 'Test'])
                sns.barplot(x='M√©trique', y='value', hue='variable', data=df_melt)
                plt.title('Comparaison des m√©triques de validation et de test')
                plt.xticks(rotation=45)
                plt.tight_layout()
                st.pyplot(fig)
        else:
            st.warning("Aucune m√©tadonn√©e de mod√®le trouv√©e.")
    else:
        st.warning("Aucun mod√®le d√©ploy√© trouv√©. Veuillez ex√©cuter le pipeline complet pour d√©ployer un mod√®le.")

# Page: Comparaison
elif page == "üîÑ Comparaison":
    st.header("üîÑ Comparaison des mod√®les")
    
    # V√©rifier si un rapport de comparaison est disponible
    deploy_path = "deploy"
    if os.path.exists(deploy_path):
        # Tenter de charger un rapport de comparaison
        comparison_path = os.path.join("build", "comparison_report.json")
        if os.path.exists(comparison_path):
            comparison = load_data(comparison_path)
            if comparison:
                # Informations sur le mod√®le actuel et v_best
                st.subheader("üìã Informations sur les mod√®les")
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**Mod√®le actuel**")
                    st.text(f"Nom: {comparison['current_model']['name']}")
                    st.text(f"Version: {comparison['current_model']['version']}")
                    st.text(f"Type: {comparison['current_model']['type']}")
                
                with col2:
                    st.markdown("**Mod√®le v_best**")
                    # Utiliser v_best_model au lieu de previous_model
                    if 'v_best_model' in comparison and comparison['v_best_model']['version']:
                        st.text(f"Version: {comparison['v_best_model']['version']}")
                    else:
                        st.text("Pas de version v_best pr√©c√©dente")
                
                # R√©sultat global de la comparaison
                st.subheader("üèÜ R√©sultat de la comparaison")
                if comparison['overall_improvement'] is None:
                    st.info("Pas de mod√®le v_best pr√©c√©dent pour comparaison")
                elif comparison['overall_improvement']:
                    st.success("‚úÖ Le mod√®le actuel est meilleur que le v_best pr√©c√©dent!")
                else:
                    st.error("‚ùå Le mod√®le actuel n'est pas meilleur que le v_best pr√©c√©dent")
                
                # Afficher le statut v_best
                if comparison.get('is_new_v_best', False):
                    st.success("üåü Ce mod√®le est devenu le nouveau v_best!")
                else:
                    st.info("üì¶ Le mod√®le v_best pr√©c√©dent a √©t√© conserv√©")
                
                # D√©tails de la comparaison des m√©triques
                if comparison['metrics_comparison']:
                    st.subheader("üìä Comparaison des m√©triques")
                    
                    # Cr√©er un DataFrame pour la comparaison
                    metrics_data = []
                    for metric, values in comparison['metrics_comparison'].items():
                        if values['v_best'] is not None:
                            metrics_data.append({
                                'M√©trique': metric,
                                'Actuel': values['current'],
                                'V_Best': values['v_best'],
                                'Diff√©rence': values['absolute_diff'],
                                'Diff√©rence (%)': values['percentage_diff'],
                                'Am√©lioration': '‚úÖ' if values['is_improvement'] else '‚ùå'
                            })
                        else:
                            # Premier mod√®le, pas de comparaison
                            metrics_data.append({
                                'M√©trique': metric,
                                'Actuel': values['current'],
                                'V_Best': 'N/A',
                                'Diff√©rence': 'N/A',
                                'Diff√©rence (%)': 'N/A',
                                'Am√©lioration': 'Premier mod√®le'
                            })
                    
                    if metrics_data:
                        df_comparison = pd.DataFrame(metrics_data)
                        st.dataframe(df_comparison)
                        
                        # Graphique de comparaison (seulement si on a des donn√©es v_best)
                        comparable_data = [row for row in metrics_data if row['V_Best'] != 'N/A']
                        if comparable_data:
                            fig, ax = plt.subplots(figsize=(10, 6))
                            df_comparable = pd.DataFrame(comparable_data)
                            df_melt = pd.melt(
                                df_comparable, 
                                id_vars=['M√©trique'], 
                                value_vars=['Actuel', 'V_Best'],
                                var_name='Version',
                                value_name='Valeur'
                            )
                            sns.barplot(x='M√©trique', y='Valeur', hue='Version', data=df_melt)
                            plt.title('Comparaison des m√©triques entre versions')
                            plt.xticks(rotation=45)
                            plt.tight_layout()
                            st.pyplot(fig)
            else:
                st.warning("Impossible de charger le rapport de comparaison.")
        else:
            st.info("Aucun rapport de comparaison disponible. Ex√©cutez le pipeline avec au moins deux versions de mod√®le pour g√©n√©rer une comparaison.")
    else:
        st.warning("Aucun mod√®le d√©ploy√© trouv√©. Veuillez ex√©cuter le pipeline complet pour d√©ployer un mod√®le.")

# Page: √Ä propos
elif page == "‚ÑπÔ∏è √Ä propos":
    st.header("‚ÑπÔ∏è √Ä propos")
    
    st.markdown("""
    ## ML Pipeline CI/CD avec syst√®me v_best
    
    Cette application fait partie d'un projet de pipeline CI/CD pour les mod√®les de machine learning.
    
    ### Fonctionnalit√©s
    
    - **Automatisation**: Construction, test, √©valuation et d√©ploiement automatis√©s des mod√®les
    - **M√©triques**: Calcul automatique des m√©triques pertinentes selon le type de mod√®le
    - **Syst√®me v_best**: Comparaison des performances et conservation du meilleur mod√®le
    - **Visualisation**: Visualisation des r√©sultats et des m√©triques
    
    ### Types de mod√®les support√©s
    
    - **R√©gression**: Mod√®les qui produisent des sorties continues
        - M√©triques: MSE, MAE, RMSE
    - **Classification**: Mod√®les qui produisent des sorties discr√®tes
        - M√©triques: Pr√©cision, Rappel, F1-score
        
    ### Pipeline GitHub Actions
    
    Le pipeline comprend les √©tapes suivantes:
    
    1. **Build**: Construction et entra√Ænement du mod√®le
    2. **Test**: Test du mod√®le sur des donn√©es non vues
    3. **Evaluate**: √âvaluation des performances du mod√®le
    4. **Compare**: Comparaison avec le mod√®le v_best
    5. **Deploy**: D√©ploiement du mod√®le depuis v_best
    
    ### Syst√®me v_best
    
    - Le conteneur `v_best` conserve toujours la meilleure version du mod√®le
    - Chaque nouveau mod√®le est compar√© avec v_best
    - Seuls les mod√®les avec >50% de m√©triques am√©lior√©es remplacent v_best
    - Le d√©ploiement se fait toujours depuis v_best
    """)

# Pied de page
st.sidebar.markdown("---")
st.sidebar.info(
    "Cette application a √©t√© d√©velopp√©e dans le cadre d'un projet acad√©mique "
    "pour d√©montrer l'automatisation des workflows de machine learning avec syst√®me v_best."
)
st.sidebar.text(f"Date: {datetime.datetime.now().strftime('%Y-%m-%d')}")