#!/usr/bin/env python3
"""
Application Streamlit pour visualiser les r√©sultats des mod√®les de machine learning.
"""
import os
import sys
import json
import base64
import datetime
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
from PIL import Image

# Configuration de la page
st.set_page_config(
    page_title="ML Pipeline - Tableau de bord",
    page_icon="üìä",
    layout="wide"
)

# Titre et description
st.title("üìä ML Pipeline - Tableau de bord")
st.write("""
Cette application pr√©sente les r√©sultats des mod√®les de machine learning ex√©cut√©s par le pipeline CI/CD.
Elle permet de visualiser les performances des mod√®les, de comparer les versions et de suivre l'√©volution des m√©triques.
""")

# Fonction pour charger les donn√©es
@st.cache_data
def load_data(path):
    """Charge les donn√©es JSON depuis un fichier."""
    try:
        with open(path, 'r') as f:
            data = json.load(f)
        return data
    except Exception as e:
        st.error(f"Erreur lors du chargement des donn√©es: {e}")
        return None

# Fonction pour convertir une image base64 en image PIL
def base64_to_image(base64_str):
    """Convertit une cha√Æne base64 en image PIL."""
    try:
        img_data = base64.b64decode(base64_str)
        return Image.open(BytesIO(img_data))
    except Exception as e:
        st.error(f"Erreur lors de la conversion de l'image: {e}")
        return None

# Barre lat√©rale pour la navigation
st.sidebar.title("Navigation")
page = st.sidebar.radio(
    "Choisir une page",
    ["üìà Tableau de bord", "üîç D√©tails du mod√®le", "üîÑ Comparaison", "‚ÑπÔ∏è √Ä propos"]
)

# Page: Tableau de bord
if page == "üìà Tableau de bord":
    st.header("üìà Tableau de bord")
    
    # Rechercher les rapports disponibles
    reports_path = "../deploy"
    if os.path.exists(reports_path):
        st.success("Mod√®les d√©ploy√©s trouv√©s!")
        
        # Charger les m√©tadonn√©es du mod√®le
        metadata = load_data(os.path.join(reports_path, "model_metadata.json"))
        if metadata:
            # Afficher les informations du mod√®le
            st.subheader("üìã Informations sur le mod√®le")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Nom du mod√®le", metadata['model_name'])
            with col2:
                st.metric("Type de mod√®le", metadata['model_type'].capitalize())
            with col3:
                st.metric("Version", metadata['model_version'])
            
            # Charger le rapport d'√©valuation
            evaluation = load_data(os.path.join(reports_path, "evaluation_report.json"))
            if evaluation:
                # Afficher les m√©triques principales
                st.subheader("üìä M√©triques principales")
                metrics = evaluation['metrics']
                
                # Cr√©er une mise en page en colonnes pour les m√©triques
                cols = st.columns(len(metrics))
                for i, (name, value) in enumerate(metrics.items()):
                    with cols[i]:
                        if name.lower() in ['mse', 'mae', 'rmse']:  # M√©triques d'erreur (plus bas = mieux)
                            st.metric(name.upper(), f"{value:.4f}", "Erreur")
                        else:  # M√©triques de performance (plus haut = mieux)
                            st.metric(name.capitalize(), f"{value:.4f}", "")
                
                # Visualisations
                if 'visualizations' in evaluation:
                    st.subheader("üìâ Visualisations")
                    visualizations = evaluation['visualizations']
                    
                    # Afficher les visualisations selon le type de mod√®le
                    if metadata['model_type'] == 'classification':
                        if 'confusion_matrix' in visualizations:
                            st.image(base64_to_image(visualizations['confusion_matrix']), caption="Matrice de confusion")
                        
                        if 'roc_curve' in visualizations and 'pr_curve' in visualizations:
                            col1, col2 = st.columns(2)
                            with col1:
                                st.image(base64_to_image(visualizations['roc_curve']), caption="Courbe ROC")
                            with col2:
                                st.image(base64_to_image(visualizations['pr_curve']), caption="Courbe Pr√©cision-Rappel")
                                
                    elif metadata['model_type'] == 'regression':
                        if 'scatter_plot' in visualizations and 'error_histogram' in visualizations:
                            col1, col2 = st.columns(2)
                            with col1:
                                st.image(base64_to_image(visualizations['scatter_plot']), caption="Pr√©dictions vs Valeurs r√©elles")
                            with col2:
                                st.image(base64_to_image(visualizations['error_histogram']), caption="Distribution des erreurs")
        else:
            st.warning("Aucune m√©tadonn√©e de mod√®le trouv√©e.")
    else:
        st.warning("Aucun mod√®le d√©ploy√© trouv√©. Veuillez ex√©cuter le pipeline complet pour d√©ployer un mod√®le.")

# Page: D√©tails du mod√®le
elif page == "üîç D√©tails du mod√®le":
    st.header("üîç D√©tails du mod√®le")
    
    # V√©rifier si un mod√®le est d√©ploy√©
    deploy_path = "../deploy"
    if os.path.exists(deploy_path):
        # Charger les m√©tadonn√©es du mod√®le
        metadata = load_data(os.path.join(deploy_path, "model_metadata.json"))
        if metadata:
            # Informations d√©taill√©es sur le mod√®le
            st.subheader("üîé Param√®tres du mod√®le")
            st.json(metadata)
            
            # Afficher les param√®tres d'entra√Ænement
            if 'training_params' in metadata:
                st.subheader("‚öôÔ∏è Param√®tres d'entra√Ænement")
                for param, value in metadata['training_params'].items():
                    st.text(f"{param}: {value}")
                    
            # Historique des m√©triques
            st.subheader("üìù Historique des m√©triques")
            if 'validation_metrics' in metadata:
                validation_metrics = metadata['validation_metrics']
                df_val = pd.DataFrame({
                    'M√©trique': list(validation_metrics.keys()),
                    'Validation': list(validation_metrics.values())
                })
                
                # Charger le rapport d'√©valuation pour les m√©triques de test
                evaluation = load_data(os.path.join(deploy_path, "evaluation_report.json"))
                if evaluation and 'metrics' in evaluation:
                    test_metrics = evaluation['metrics']
                    df_val['Test'] = [test_metrics.get(metric, None) for metric in validation_metrics.keys()]
                
                # Afficher le tableau des m√©triques
                st.dataframe(df_val)
                
                # Graphique des m√©triques
                fig, ax = plt.subplots(figsize=(10, 6))
                df_melt = pd.melt(df_val, id_vars=['M√©trique'], value_vars=['Validation', 'Test'])
                sns.barplot(x='M√©trique', y='value', hue='variable', data=df_melt)
                plt.title('Comparaison des m√©triques de validation et de test')
                plt.xticks(rotation=45)
                plt.tight_layout()
                st.pyplot(fig)
        else:
            st.warning("Aucune m√©tadonn√©e de mod√®le trouv√©e.")
    else:
        st.warning("Aucun mod√®le d√©ploy√© trouv√©. Veuillez ex√©cuter le pipeline complet pour d√©ployer un mod√®le.")

# Page: Comparaison
elif page == "üîÑ Comparaison":
    st.header("üîÑ Comparaison des mod√®les")
    
    # V√©rifier si un rapport de comparaison est disponible
    deploy_path = "../deploy"
    if os.path.exists(deploy_path):
        # Tenter de charger un rapport de comparaison
        comparison_path = os.path.join("../build", "comparison_report.json")
        if os.path.exists(comparison_path):
            comparison = load_data(comparison_path)
            if comparison:
                # Informations sur le mod√®le actuel et pr√©c√©dent
                st.subheader("üìã Informations sur les mod√®les")
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**Mod√®le actuel**")
                    st.text(f"Nom: {comparison['current_model']['name']}")
                    st.text(f"Version: {comparison['current_model']['version']}")
                    st.text(f"Type: {comparison['current_model']['type']}")
                
                with col2:
                    st.markdown("**Mod√®le pr√©c√©dent**")
                    if comparison['previous_model']['version']:
                        st.text(f"Version: {comparison['previous_model']['version']}")
                    else:
                        st.text("Pas de version pr√©c√©dente")
                
                # R√©sultat global de la comparaison
                st.subheader("üèÜ R√©sultat de la comparaison")
                if comparison['overall_improvement'] is None:
                    st.info("Pas de mod√®le pr√©c√©dent pour comparaison")
                elif comparison['overall_improvement']:
                    st.success("‚úÖ Le mod√®le actuel est meilleur que le pr√©c√©dent!")
                else:
                    st.error("‚ùå Le mod√®le actuel n'est pas meilleur que le pr√©c√©dent")
                
                # D√©tails de la comparaison des m√©triques
                if comparison['metrics_comparison']:
                    st.subheader("üìä Comparaison des m√©triques")
                    
                    # Cr√©er un DataFrame pour la comparaison
                    metrics_data = []
                    for metric, values in comparison['metrics_comparison'].items():
                        if values['previous'] is not None:
                            metrics_data.append({
                                'M√©trique': metric,
                                'Actuel': values['current'],
                                'Pr√©c√©dent': values['previous'],
                                'Diff√©rence': values['absolute_diff'],
                                'Diff√©rence (%)': values['percentage_diff'],
                                'Am√©lioration': '‚úÖ' if values['is_improvement'] else '‚ùå'
                            })
                    
                    if metrics_data:
                        df_comparison = pd.DataFrame(metrics_data)
                        st.dataframe(df_comparison)
                        
                        # Graphique de comparaison
                        fig, ax = plt.subplots(figsize=(10, 6))
                        df_melt = pd.melt(
                            df_comparison, 
                            id_vars=['M√©trique'], 
                            value_vars=['Actuel', 'Pr√©c√©dent'],
                            var_name='Version',
                            value_name='Valeur'
                        )
                        sns.barplot(x='M√©trique', y='Valeur', hue='Version', data=df_melt)
                        plt.title('Comparaison des m√©triques entre versions')
                        plt.xticks(rotation=45)
                        plt.tight_layout()
                        st.pyplot(fig)
            else:
                st.warning("Impossible de charger le rapport de comparaison.")
        else:
            st.info("Aucun rapport de comparaison disponible. Ex√©cutez le pipeline avec au moins deux versions de mod√®le pour g√©n√©rer une comparaison.")
    else:
        st.warning("Aucun mod√®le d√©ploy√© trouv√©. Veuillez ex√©cuter le pipeline complet pour d√©ployer un mod√®le.")

# Page: √Ä propos
elif page == "‚ÑπÔ∏è √Ä propos":
    st.header("‚ÑπÔ∏è √Ä propos")
    
    st.markdown("""
    ## ML Pipeline CI/CD
    
    Cette application fait partie d'un projet de pipeline CI/CD pour les mod√®les de machine learning.
    
    ### Fonctionnalit√©s
    
    - **Automatisation**: Construction, test, √©valuation et d√©ploiement automatis√©s des mod√®les
    - **M√©triques**: Calcul automatique des m√©triques pertinentes selon le type de mod√®le
    - **Comparaison**: Comparaison des performances entre diff√©rentes versions des mod√®les
    - **Visualisation**: Visualisation des r√©sultats et des m√©triques
    
    ### Types de mod√®les support√©s
    
    - **R√©gression**: Mod√®les qui produisent des sorties continues
        - M√©triques: MSE, MAE, RMSE
    - **Classification**: Mod√®les qui produisent des sorties discr√®tes
        - M√©triques: Pr√©cision, Rappel, F1-score
        
    ### Pipeline GitHub Actions
    
    Le pipeline comprend les √©tapes suivantes:
    
    1. **Build**: Construction et entra√Ænement du mod√®le
    2. **Test**: Test du mod√®le sur des donn√©es non vues
    3. **Evaluate**: √âvaluation des performances du mod√®le
    4. **Compare**: Comparaison avec la version pr√©c√©dente
    5. **Deploy**: D√©ploiement du mod√®le s'il y a une am√©lioration
    """)

# Pied de page
st.sidebar.markdown("---")
st.sidebar.info(
    "Cette application a √©t√© d√©velopp√©e dans le cadre d'un projet acad√©mique "
    "pour d√©montrer l'automatisation des workflows de machine learning."
)
st.sidebar.text(f"Date: {datetime.datetime.now().strftime('%Y-%m-%d')}")